title,domain,difficulty,blurb,angles,seed_context,tags
Balancing Precision and Recall,People,Beginner,Explores the trade-off between sensitivity and specificity in classification. Emphasizes cost-aware thresholding and domain goals.,"[""What costs matter most?"",""Which errors are tolerable?"",""How should thresholds adapt?""]",Precision = true positives among predicted positives; recall = true positives among actual positives.\nClass imbalance and thresholds jointly shape both metrics.,"metrics, classification, evaluation"
External Validity vs Internal Validity,People,Beginner,Shows how experimental control can conflict with generalizability. Offers strategies to balance rigor with relevance.,"[""When to prioritize control?"",""What makes results travel?"",""Can designs hybridize?""]",Internal validity concerns causal identification within a study.\nExternal validity concerns applicability beyond the sample or setting.,"causality, design, generalization"
Trade-offs in Resource Allocation,People,Beginner,Optimizing limited resources under competing objectives. Introduces opportunity costs and Pareto efficiency.,"[""Whose objective counts?"",""Where is the margin?"",""What constraints bind?""]",Resources are finite and choices impose opportunity costs.\nPareto improvements make someone better off without making others worse off.,"optimization, policy, efficiency"
Measuring Causal Effects with Instrumental Variables,People,Beginner,"When experiments are infeasible, instruments can identify local causal effects. Discusses validity conditions and limitations.","[""Is the instrument valid?"",""What is LATE telling us?"",""How local is local?""]",An instrument affects treatment but not the outcome except via treatment.\nAssumptions include relevance and exclusion restrictions.,"causality, IV, identification"
Interpreting Long-Tail Distributions,People,Beginner,Many phenomena follow heavy-tailed patterns with rare but extreme events. Explores implications for risk and estimation.,"[""What is typical vs average?"",""Are outliers signal?"",""How to hedge tails?""]","Heavy tails imply higher probability of extreme values than normal.\nSample means may be unstable, and variance may be large or infinite.","risk, statistics, distributions"
Robustness Checks in Empirical Studies,People,Beginner,Robustness analysis tests whether findings persist across reasonable alternatives. Clarifies what counts as informative variation.,"[""Which perturbations matter?"",""When is fragility a clue?"",""How to avoid p-hacking?""]","Analysts vary specifications, samples, and measures to test stability.\nTransparency in protocols improves credibility.","replicability, methods, reliability"
"Confounding, Mediation, and Moderation",People,Beginner,"Differentiates confounders, mediators, and moderators in causal models. Highlights implications for estimation and interpretation.","[""What to adjust for?"",""When to stratify?"",""How to avoid collider bias?""]",Confounders affect both treatment and outcome.\nMediators transmit effects; moderators alter effect sizes across groups.,"causality, modeling, bias"
The Replication Crisis and Its Remedies,People,Beginner,"Surveys failed replications across fields and proposed reforms. Emphasizes incentives, transparency, and measurement quality.","[""Which reforms scale?"",""What counts as success?"",""How to reward rigor?""]","Large-scale projects have found lower-than-expected replication rates.\nPre-registration, data sharing, and larger samples are common remedies.","science, replication, incentives"
Algorithmic Fairness Metrics Compared,People,Beginner,Fairness has multiple incompatible definitions. Compares metrics and their policy trade-offs.,"[""Which metric fits goals?"",""What harms are salient?"",""Can parity be calibrated?""]","Equalized odds, demographic parity, and calibration can conflict.\nChoice of metric encodes normative commitments.","fairness, algorithms, ethics"
Modeling Nonlinear Dynamics,People,Beginner,Systems often show nonlinear behaviors like thresholds and feedback. Presents modeling tools and interpretive cautions.,"[""Where are the thresholds?"",""Is feedback stabilizing?"",""Can simple models suffice?""]","Nonlinear models capture interactions and feedback loops.\nSmall changes can produce large, path-dependent effects.","modeling, systems, complexity"
The Value of Null Results,People,Beginner,Null findings can inform theories and practice when interpreted correctly. Discusses publication bias and evidentiary value.,"[""When is null informative?"",""How to predefine MDE?"",""Can nulls cumulate?""]",Publication bias favors significant results.\nPower analyses and registered reports elevate the role of null findings.,"evidence, bias, publication"
Scalability vs Interpretability,People,Beginner,Large models scale performance but often at the cost of transparency. Weighs practical gains against explainability.,"[""What needs to be explained?"",""Who bears risk?"",""Is a smaller model enough?""]",Model size can improve accuracy on benchmarks.\nStakeholders may require explanations for audit and trust.,"AI, models, trade-offs"
Missing Data Mechanisms and Bias,People,Beginner,"Missingness mechanisms shape bias and valid inferences. Reviews MCAR, MAR, and MNAR with practical strategies.","[""What drives missingness?"",""Impute or model?"",""Sensitivity to assumptions?""]",MCAR: unrelated; MAR: related to observed; MNAR: related to unobserved.\nApproaches include multiple imputation and selection models.,"data, bias, inference"
Survey Design: Wording and Order Effects,People,Beginner,Question wording and sequence can shift responses. Outlines design principles to reduce measurement error.,"[""Which terms confuse?"",""What should precede what?"",""Pretest or pilot?""]",Cognitive load and priming can bias answers.\nRandomization and careful pretesting mitigate effects.,"surveys, measurement, design"
Forecasting Under Structural Breaks,People,Beginner,Models trained on past regimes can fail after breaks. Introduces diagnostics and adaptive methods.,"[""Detect or assume?"",""How fast to adapt?"",""Diversify models?""]",Structural breaks change relationships over time.\nRolling windows and regime-switching models address instability.,"time series, forecasting, change"
Uncertainty Communication for Decision-Making,People,Beginner,"Conveys how to present uncertainty without undermining action. Focuses on formats, framing, and audience needs.","[""Ranges or probabilities?"",""What to visualize?"",""When to say unknown?""]","Decision-makers need calibrated, comprehensible uncertainty.\nVisual aids and verbal scales can complement numeric summaries.","risk, communication, decisions"
Priors as Encoded Knowledge,People,Beginner,"In Bayesian analysis, priors formalize background beliefs. Explores elicitation and sensitivity.","[""Whose prior is it?"",""Weakly vs informative?"",""Robust to misspecification?""]",Priors combine with likelihoods via Bayes' rule.\nSensitivity analysis checks how conclusions vary with the prior.,"Bayesian, inference, modeling"
Cost-Benefit Analysis Under Ambiguity,People,Beginner,"When probabilities are imprecise, standard CBA falters. Examines ambiguity-aware frameworks.","[""Maximin or average?"",""Whose risk aversion?"",""Value of information?""]",Knightian uncertainty implies unknown probabilities.\nRobust decision criteria and VOI can guide choices.,"economics, policy, uncertainty"
Heterogeneous Treatment Effects,People,Beginner,Average effects can conceal variation across units. Surveys methods to estimate and interpret heterogeneity.,"[""Which subgroups matter?"",""Risk of false discovery?"",""Global vs local effects?""]",Methods include CATE models and subgroup analysis.\nMultiple testing and overfitting threaten validity.,"causality, heterogeneity, methods"
Counterfactual Thinking in Policy Design,People,Beginner,Policies should be evaluated against plausible alternatives. Encourages explicit counterfactuals and constraints.,"[""Compared to what?"",""Feasible alternatives?"",""Unintended consequences?""]","Counterfactuals define causal effects.\nPolicy evaluation requires realistic comparisons, not perfection.","policy, evaluation, causality"
Sample Size Planning and Power,People,Beginner,Power analysis helps ensure studies can detect meaningful effects. Covers MDE and resource constraints.,"[""What effect matters?"",""Type I vs II costs?"",""Sequential designs?""]","Power depends on effect size, variance, alpha, and N.\nPlanning balances feasibility with inferential goals.","design, power, statistics"
Open Science Practices That Stick,People,Beginner,Sustainable open practices rely on incentives and tooling. Focuses on pragmatic adoption.,"[""Which norms are cheap?"",""Automate or educate?"",""How to credit sharing?""]","Open data, code, and protocols improve reuse.\nBadges, registries, and CI tools lower friction.","open science, incentives, reproducibility"
Human-in-the-Loop Systems,People,Beginner,Combining algorithmic automation with expert oversight can improve outcomes. Discusses division of labor and failure modes.,"[""Where to insert humans?"",""Feedback loops harmful?"",""Design for override?""]",Automation excels at scale; humans bring judgment.\nInterfaces and incentives shape collaboration quality.,"AI, systems, design"
Signal vs Noise in High-Dimensional Data,People,Beginner,"As dimensions grow, spurious patterns proliferate. Reviews regularization and validation.","[""How to control overfit?"",""What to hold out?"",""Interpretability trade-offs?""]",High p/n ratios raise variance and false discoveries.\nTechniques include cross-validation and penalization.,"statistics, ML, overfitting"
Ethical Use of A/B Testing,People,Beginner,Experiments can create risk when deployed at scale. Articulates guardrails and consent models.,"[""When is consent needed?"",""Stop for harm?"",""Equity across arms?""]","A/B tests randomize users to variants.\nOversight, monitoring, and debriefing can mitigate ethical concerns.","ethics, experiments, product"
When Correlation Is Useful,People,Beginner,"Correlation does not imply causation, yet remains informative. Distinguishes prediction, description, and explanation.","[""What is the goal?"",""Can proxies suffice?"",""Risks of misinterpretation?""]","Correlation quantifies linear association.\nUseful for screening, QC, and forecasting when causality is not needed.","statistics, correlation, prediction"
Triangulation Across Methods,People,Beginner,Using multiple methods can strengthen conclusions. Highlights complementarity and convergence.,"[""Which methods complement?"",""Resolve contradictions?"",""Cost vs payoff?""]",Qualitative and quantitative approaches offer different lenses.\nConvergence across methods boosts credibility.,"methods, mixed methods, validity"
Interpreting p-values and Alternatives,People,Beginner,"Clarifies what p-values do and do not mean. Surveys alternatives like intervals, Bayes factors, and S-values.","[""What is evidence?"",""Thresholds or gradients?"",""Report what else?""]",A p-value is the probability of data as extreme under H0.\nComplementary summaries can aid interpretation.,"inference, statistics, evidence"
Sensitivity Analysis That Matters,People,Beginner,"Assesses how conclusions change under plausible deviations. Prioritizes transparent, decision-relevant variations.","[""Which knobs to turn?"",""Range vs realism?"",""Communicate clearly?""]","Sensitivity checks vary assumptions and inputs.\nFocus on influential, justified perturbations linked to decisions.","robustness, decisions, modeling"
Transfer Learning Across Contexts,People,Beginner,Models trained in one domain can adapt elsewhere with care. Discusses domain shift and fine-tuning.,"[""What transfers well?"",""Detect shift early?"",""Risk of negative transfer?""]","Distribution shift reduces performance when context changes.\nFine-tuning with small, labeled data can recover accuracy.","ML, generalization, domain shift"
"Balancing Precision and Recall","{DOMAIN}","{DIFFICULTY}","Explores the trade-off between sensitivity and specificity in classification. Emphasizes cost-aware thresholding and domain goals.","[""What costs matter most?"",""Which errors are tolerable?"",""How should thresholds adapt?""]","Precision = true positives among predicted positives; recall = true positives among actual positives.\nClass imbalance and thresholds jointly shape both metrics.","metrics, classification, evaluation"
"External Validity vs Internal Validity","{DOMAIN}","{DIFFICULTY}","Shows how experimental control can conflict with generalizability. Offers strategies to balance rigor with relevance.","[""When to prioritize control?"",""What makes results travel?"",""Can designs hybridize?""]","Internal validity concerns causal identification within a study.\nExternal validity concerns applicability beyond the sample or setting.","causality, design, generalization"
"Trade-offs in Resource Allocation","{DOMAIN}","{DIFFICULTY}","Optimizing limited resources under competing objectives. Introduces opportunity costs and Pareto efficiency.","[""Whose objective counts?"",""Where is the margin?"",""What constraints bind?""]","Resources are finite and choices impose opportunity costs.\nPareto improvements make someone better off without making others worse off.","optimization, policy, efficiency"
"Measuring Causal Effects with Instrumental Variables","{DOMAIN}","{DIFFICULTY}","When experiments are infeasible, instruments can identify local causal effects. Discusses validity conditions and limitations.","[""Is the instrument valid?"",""What is LATE telling us?"",""How local is local?""]","An instrument affects treatment but not the outcome except via treatment.\nAssumptions include relevance and exclusion restrictions.","causality, IV, identification"
"Interpreting Long-Tail Distributions","{DOMAIN}","{DIFFICULTY}","Many phenomena follow heavy-tailed patterns with rare but extreme events. Explores implications for risk and estimation.","[""What is typical vs average?"",""Are outliers signal?"",""How to hedge tails?""]","Heavy tails imply higher probability of extreme values than normal.\nSample means may be unstable, and variance may be large or infinite.","risk, statistics, distributions"
"Robustness Checks in Empirical Studies","{DOMAIN}","{DIFFICULTY}","Robustness analysis tests whether findings persist across reasonable alternatives. Clarifies what counts as informative variation.","[""Which perturbations matter?"",""When is fragility a clue?"",""How to avoid p-hacking?""]","Analysts vary specifications, samples, and measures to test stability.\nTransparency in protocols improves credibility.","replicability, methods, reliability"
"Confounding, Mediation, and Moderation","{DOMAIN}","{DIFFICULTY}","Differentiates confounders, mediators, and moderators in causal models. Highlights implications for estimation and interpretation.","[""What to adjust for?"",""When to stratify?"",""How to avoid collider bias?""]","Confounders affect both treatment and outcome.\nMediators transmit effects; moderators alter effect sizes across groups.","causality, modeling, bias"
"The Replication Crisis and Its Remedies","{DOMAIN}","{DIFFICULTY}","Surveys failed replications across fields and proposed reforms. Emphasizes incentives, transparency, and measurement quality.","[""Which reforms scale?"",""What counts as success?"",""How to reward rigor?""]","Large-scale projects have found lower-than-expected replication rates.\nPre-registration, data sharing, and larger samples are common remedies.","science, replication, incentives"
"Algorithmic Fairness Metrics Compared","{DOMAIN}","{DIFFICULTY}","Fairness has multiple incompatible definitions. Compares metrics and their policy trade-offs.","[""Which metric fits goals?"",""What harms are salient?"",""Can parity be calibrated?""]","Equalized odds, demographic parity, and calibration can conflict.\nChoice of metric encodes normative commitments.","fairness, algorithms, ethics"
"Modeling Nonlinear Dynamics","{DOMAIN}","{DIFFICULTY}","Systems often show nonlinear behaviors like thresholds and feedback. Presents modeling tools and interpretive cautions.","[""Where are the thresholds?"",""Is feedback stabilizing?"",""Can simple models suffice?""]","Nonlinear models capture interactions and feedback loops.\nSmall changes can produce large, path-dependent effects.","modeling, systems, complexity"
"The Value of Null Results","{DOMAIN}","{DIFFICULTY}","Null findings can inform theories and practice when interpreted correctly. Discusses publication bias and evidentiary value.","[""When is null informative?"",""How to predefine MDE?"",""Can nulls cumulate?""]","Publication bias favors significant results.\nPower analyses and registered reports elevate the role of null findings.","evidence, bias, publication"
"Scalability vs Interpretability","{DOMAIN}","{DIFFICULTY}","Large models scale performance but often at the cost of transparency. Weighs practical gains against explainability.","[""What needs to be explained?"",""Who bears risk?"",""Is a smaller model enough?""]","Model size can improve accuracy on benchmarks.\nStakeholders may require explanations for audit and trust.","AI, models, trade-offs"
"Missing Data Mechanisms and Bias","{DOMAIN}","{DIFFICULTY}","Missingness mechanisms shape bias and valid inferences. Reviews MCAR, MAR, and MNAR with practical strategies.","[""What drives missingness?"",""Impute or model?"",""Sensitivity to assumptions?""]","MCAR: unrelated; MAR: related to observed; MNAR: related to unobserved.\nApproaches include multiple imputation and selection models.","data, bias, inference"
"Survey Design: Wording and Order Effects","{DOMAIN}","{DIFFICULTY}","Question wording and sequence can shift responses. Outlines design principles to reduce measurement error.","[""Which terms confuse?"",""What should precede what?"",""Pretest or pilot?""]","Cognitive load and priming can bias answers.\nRandomization and careful pretesting mitigate effects.","surveys, measurement, design"
"Forecasting Under Structural Breaks","{DOMAIN}","{DIFFICULTY}","Models trained on past regimes can fail after breaks. Introduces diagnostics and adaptive methods.","[""Detect or assume?"",""How fast to adapt?"",""Diversify models?""]","Structural breaks change relationships over time.\nRolling windows and regime-switching models address instability.","time series, forecasting, change"
"Uncertainty Communication for Decision-Making","{DOMAIN}","{DIFFICULTY}","Conveys how to present uncertainty without undermining action. Focuses on formats, framing, and audience needs.","[""Ranges or probabilities?"",""What to visualize?"",""When to say unknown?""]","Decision-makers need calibrated, comprehensible uncertainty.\nVisual aids and verbal scales can complement numeric summaries.","risk, communication, decisions"
"Priors as Encoded Knowledge","{DOMAIN}","{DIFFICULTY}","In Bayesian analysis, priors formalize background beliefs. Explores elicitation and sensitivity.","[""Whose prior is it?"",""Weakly vs informative?"",""Robust to misspecification?""]","Priors combine with likelihoods via Bayes' rule.\nSensitivity analysis checks how conclusions vary with the prior.","Bayesian, inference, modeling"
"Cost-Benefit Analysis Under Ambiguity","{DOMAIN}","{DIFFICULTY}","When probabilities are imprecise, standard CBA falters. Examines ambiguity-aware frameworks.","[""Maximin or average?"",""Whose risk aversion?"",""Value of information?""]","Knightian uncertainty implies unknown probabilities.\nRobust decision criteria and VOI can guide choices.","economics, policy, uncertainty"
"Heterogeneous Treatment Effects","{DOMAIN}","{DIFFICULTY}","Average effects can conceal variation across units. Surveys methods to estimate and interpret heterogeneity.","[""Which subgroups matter?"",""Risk of false discovery?"",""Global vs local effects?""]","Methods include CATE models and subgroup analysis.\nMultiple testing and overfitting threaten validity.","causality, heterogeneity, methods"
"Counterfactual Thinking in Policy Design","{DOMAIN}","{DIFFICULTY}","Policies should be evaluated against plausible alternatives. Encourages explicit counterfactuals and constraints.","[""Compared to what?"",""Feasible alternatives?"",""Unintended consequences?""]","Counterfactuals define causal effects.\nPolicy evaluation requires realistic comparisons, not perfection.","policy, evaluation, causality"
"Sample Size Planning and Power","{DOMAIN}","{DIFFICULTY}","Power analysis helps ensure studies can detect meaningful effects. Covers MDE and resource constraints.","[""What effect matters?"",""Type I vs II costs?"",""Sequential designs?""]","Power depends on effect size, variance, alpha, and N.\nPlanning balances feasibility with inferential goals.","design, power, statistics"
"Open Science Practices That Stick","{DOMAIN}","{DIFFICULTY}","Sustainable open practices rely on incentives and tooling. Focuses on pragmatic adoption.","[""Which norms are cheap?"",""Automate or educate?"",""How to credit sharing?""]","Open data, code, and protocols improve reuse.\nBadges, registries, and CI tools lower friction.","open science, incentives, reproducibility"
"Human-in-the-Loop Systems","{DOMAIN}","{DIFFICULTY}","Combining algorithmic automation with expert oversight can improve outcomes. Discusses division of labor and failure modes.","[""Where to insert humans?"",""Feedback loops harmful?"",""Design for override?""]","Automation excels at scale; humans bring judgment.\nInterfaces and incentives shape collaboration quality.","AI, systems, design"
"Signal vs Noise in High-Dimensional Data","{DOMAIN}","{DIFFICULTY}","As dimensions grow, spurious patterns proliferate. Reviews regularization and validation.","[""How to control overfit?"",""What to hold out?"",""Interpretability trade-offs?""]","High p/n ratios raise variance and false discoveries.\nTechniques include cross-validation and penalization.","statistics, ML, overfitting"
"Ethical Use of A/B Testing","{DOMAIN}","{DIFFICULTY}","Experiments can create risk when deployed at scale. Articulates guardrails and consent models.","[""When is consent needed?"",""Stop for harm?"",""Equity across arms?""]","A/B tests randomize users to variants.\nOversight, monitoring, and debriefing can mitigate ethical concerns.","ethics, experiments, product"
"When Correlation Is Useful","{DOMAIN}","{DIFFICULTY}","Correlation does not imply causation, yet remains informative. Distinguishes prediction, description, and explanation.","[""What is the goal?"",""Can proxies suffice?"",""Risks of misinterpretation?""]","Correlation quantifies linear association.\nUseful for screening, QC, and forecasting when causality is not needed.","statistics, correlation, prediction"
"Triangulation Across Methods","{DOMAIN}","{DIFFICULTY}","Using multiple methods can strengthen conclusions. Highlights complementarity and convergence.","[""Which methods complement?"",""Resolve contradictions?"",""Cost vs payoff?""]","Qualitative and quantitative approaches offer different lenses.\nConvergence across methods boosts credibility.","methods, mixed methods, validity"
"Interpreting p-values and Alternatives","{DOMAIN}","{DIFFICULTY}","Clarifies what p-values do and do not mean. Surveys alternatives like intervals, Bayes factors, and S-values.","[""What is evidence?"",""Thresholds or gradients?"",""Report what else?""]","A p-value is the probability of data as extreme under H0.\nComplementary summaries can aid interpretation.","inference, statistics, evidence"
"Sensitivity Analysis That Matters","{DOMAIN}","{DIFFICULTY}","Assesses how conclusions change under plausible deviations. Prioritizes transparent, decision-relevant variations.","[""Which knobs to turn?"",""Range vs realism?"",""Communicate clearly?""]","Sensitivity checks vary assumptions and inputs.\nFocus on influential, justified perturbations linked to decisions.","robustness, decisions, modeling"
"Transfer Learning Across Contexts","{DOMAIN}","{DIFFICULTY}","Models trained in one domain can adapt elsewhere with care. Discusses domain shift and fine-tuning.","[""What transfers well?"",""Detect shift early?"",""Risk of negative transfer?""]","Distribution shift reduces performance when context changes.\nFine-tuning with small, labeled data can recover accuracy.","ML, generalization, domain shift"